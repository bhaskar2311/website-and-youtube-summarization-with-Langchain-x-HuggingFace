{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3fd6a3",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b59001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51f8b3",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14a1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229af45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "c:\\Users\\bhask\\Udemy Krish Naik\\GenAI Course\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, task='text-generation')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",  # ✅ specify the task\n",
    "    max_length=150,\n",
    "    temperature=0.7,\n",
    "    token=os.getenv(\"HF_TOKEN\")  # Make sure your HF_TOKEN is set correctly\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebbb101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhask\\Udemy Krish Naik\\GenAI Course\\Langchain\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
      "\n",
      "The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\n",
      "\n",
      "Machine learning can be categorized into three types:\n",
      "\n",
      "1. Supervised learning: where the computer is provided with example inputs and their desired outputs, given by a “teacher”, the computer learns to produce the correct output to new inputs. It infers a function from example input-output pairs.\n",
      "\n",
      "2. Unsupervised learning: where the computer is not given the desired output and has to figure it out on its own. The computer will learn to find patterns and structure in the data.\n",
      "\n",
      "3. Semi-supervised learning: where the computer is given both labelled and unlabelled data for learning.\n",
      "\n",
      "Some of the most common applications of machine learning include email filtering, detection of network intruders, and computer vision. Machine learning is also used in recommendation systems, like the ones used by Amazon, Netflix, and Google.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is machine learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83e2771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? Generative AI refers to a type of artificial intelligence that can create new content, such as text, images, or music, based on patterns it has learned from existing data. This is done by training AI models on large datasets and using algorithms to generate new outputs that are similar in style and structure to the training data.\n",
      "\n",
      "Generative AI models can be used for a variety of tasks, such as creating realistic images, generating text for articles or social media posts, or even composing music. They are becoming increasingly popular in industries such as marketing, entertainment, and design, as they can help automate content creation and reduce the need for human labor.\n",
      "\n",
      "Examples of generative AI include deep learning models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as well as language models like BERT and GPT-3. These models are trained on large amounts of data and can generate new content that is similar in style and quality to the training data.\n",
      "\n",
      "Generative AI has many potential applications, such as creating personalized content for users, generating synthetic data for training other AI models, and even creating art and music. However, it also raises ethical and technical challenges, such as ensuring that the AI models do not generate harmful or inappropriate content, and ensuring that the AI models do not infringe on copyrights or other intellectual property rights.\n",
      "\n",
      "Overall, generative AI is an exciting and rapidly developing field that has the potential to revolutionize many industries and change the way we create and consume content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhask\\Udemy Krish Naik\\GenAI Course\\Langchain\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is generative AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0680d1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template=' \\nQuestion : {question}\\nAnswer : Lets think step by step\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"\"\" \n",
    "Question : {question}\n",
    "Answer : Lets think step by step\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8809a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhask\\AppData\\Local\\Temp\\ipykernel_3496\\3565696600.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm,prompt=prompt)\n",
      "c:\\Users\\bhask\\Udemy Krish Naik\\GenAI Course\\Langchain\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" and when? Australia won the Cricket World Cup 2011 on April 2, 2011, defeating New Zealand in the final match held at the Wankhede Stadium in Mumbai, India. The match went into a super over due to a tie in the regular play, and Australia won the super over by scoring 6 runs and restricting New Zealand to 5 runs. This was Australia's fourth World Cup victory.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket world cup 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d551330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
